{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vsa import VSA\n",
    "VSA.mode = \"SOFTWARE\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "vec = VSA.random(size = (100, 1000), device = device)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler('./profiler/tests'),\n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_stack=True) as prof:\n",
    "    out = VSA.multibind(vec)\n",
    "\n",
    "print(prof.key_averages().table(row_limit=10))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsa.bind(vectors[0], vectors[1]), vsa.bundle(vectors[0], vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsa.multibind(vectors), vsa.multiset(vectors, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsa.similarity(vectors[0], vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "weights = torch.tensor([1,2,3,4,-1])\n",
    "torch.matmul(weights.type(torch.LongTensor), vectors.type(torch.LongTensor))\n",
    "# weights * vectors.transpose(-2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([[1,2,3,4,-1], [-1,-2,-3, -4, 1]])\n",
    "vectors.unsqueeze(0).repeat(2,1,1)\n",
    "torch.matmul(weights.type(torch.LongTensor), vectors.type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import algo1, algo2, algo3\n",
    "from vsa.vsa import VSA\n",
    "from vsa.resonator import Resonator\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"tests/testing/\")\n",
    "\n",
    "VSA_MODE = \"HARDWARE\"\n",
    "NUM_VEC_SUPERPOSED = 2\n",
    "ALGO = \"ALGO3\" # ALGO1, ALGO2, ALGO3\n",
    "TRIALS = 20    # for ALGO2\n",
    "DIM = 2000\n",
    "FACTORS = 5\n",
    "CODEVECTORS = 10\n",
    "CODEVECTORS : tuple = (3,3,3,10,2)\n",
    "NOISE_LEVEL = 0.0  # compositional vector noise\n",
    "ITERATIONS = 5000    # max number of iterations for factorization\n",
    "NORMALIZE = True   # for SOFTWARE mode. Normalize the initial estimate and the input vector (when the input is a bundled vector)\n",
    "ACTIVATION = 'EXPO'  # 'NONE', 'ABS', 'NONNEG'\n",
    "RESONATOR_TYPE = \"SEQUENTIAL\" # \"CONCURRENT\", \"SEQUENTIAL\"\n",
    "ARGMAX_ABS = True\n",
    "REORDER_CODEBOOKS = False # True False\n",
    "if VSA_MODE == 'HARDWARE':\n",
    "    NORMALIZE = None\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "vsa = VSA(\n",
    "    root=\"tests/testing/\",\n",
    "    mode=VSA_MODE,\n",
    "    dim=DIM,\n",
    "    num_factors=FACTORS,\n",
    "    num_codevectors=CODEVECTORS,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "rn = Resonator(vsa, type=RESONATOR_TYPE, activation=ACTIVATION, iterations=ITERATIONS, argmax_abs=ARGMAX_ABS)\n",
    "ds = VSADataset(\"tests/testing/\", 1, vsa, algo=ALGO, num_vectors_superposed=NUM_VEC_SUPERPOSED, noise=NOISE_LEVEL)\n",
    "# Test sample\n",
    "label = [(0, 2, 1, 1), (1, 2, 1, 5)]\n",
    "# label = [(0, 0, 2, 3), (2, 0, 0, 3)]\n",
    "vector = ds.lookup_algo3(label)\n",
    "\n",
    "labels = [label]\n",
    "vectors = vector.repeat(BATCH_SIZE, 1)\n",
    "\n",
    "codebooks = None\n",
    "orig_indices = None\n",
    "\n",
    "if REORDER_CODEBOOKS:\n",
    "    codebooks, orig_indices = rn.reorder_codebooks(codebooks)\n",
    "\n",
    "init_estimates = rn.get_init_estimates(codebooks, BATCH_SIZE)\n",
    "if NORMALIZE:\n",
    "    init_estimates = vsa.normalize(init_estimates)\n",
    "\n",
    "if (NUM_VEC_SUPERPOSED == 1):\n",
    "    outcome, iter = rn(vectors, init_estimates, codebooks, orig_indices)\n",
    "    # Make them the same format as multi-vector for easier parsing\n",
    "    outcomes = [[outcome[x]] for x in range(BATCH_SIZE)]\n",
    "    convergences = [1 if iter == ITERATIONS-1 else 0] * BATCH_SIZE\n",
    "    iters = [iter] * BATCH_SIZE\n",
    "else:\n",
    "    if ALGO == \"ALGO1\":\n",
    "        outcomes, convergences, iters = algo1(vsa, rn, vectors, init_estimates, codebooks, orig_indices, NORMALIZE)\n",
    "    elif ALGO == \"ALGO2\":\n",
    "        outcomes, convergences, iters = algo2(vsa, rn, vectors, DIM, FACTORS, codebooks, orig_indices, NORMALIZE)\n",
    "    elif ALGO == \"ALGO3\":\n",
    "        outcomes, convergences, iters = algo3(vsa, rn, vectors, NORMALIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(outcomes)\n",
    "print(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vsa.vsa_tensor import VSATensor\n",
    "from vsa import vsa\n",
    "import torch\n",
    "\n",
    "VSATensor.set_mode(\"SOFTWARE\")\n",
    "x = VSATensor.random(1, 100)\n",
    "print(x)\n",
    "print(x.quantized)\n",
    "y = VSATensor.random(2, 100)\n",
    "# z = vsa.bind(x, y)\n",
    "# print(z)\n",
    "# print(z.quantized)\n",
    "# z = vsa.multibind(y)\n",
    "# print(z)\n",
    "# print(z.quantized)\n",
    "\n",
    "b = vsa.bundle(x, y)\n",
    "print(b)\n",
    "print(b.quantized)\n",
    "c = vsa.bundle(x, y, True)\n",
    "print(c)\n",
    "print(c.quantized)\n",
    "print(vsa.dot_similarity(x, c))\n",
    "print(vsa.dot_similarity(c, c))\n",
    "print(vsa.dot_similarity(b, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CA90 distributive property over bind and permute\n",
    "import torch\n",
    "from vsa import VSA\n",
    "VSA.mode = \"HARDWARE\"\n",
    "vecs_f1 = VSA.random(size = (2, 64))\n",
    "vecs_f2 = VSA.ca90(vecs_f1)\n",
    "vecs_f3 = VSA.ca90(vecs_f2)\n",
    "bound_f1 = VSA.multibind(vecs_f1)\n",
    "bound_f2 = VSA.multibind(vecs_f2)\n",
    "bound_f3 = VSA.multibind(vecs_f3)\n",
    "permute_f1 = VSA.permute(vecs_f1)\n",
    "permute_f2 = VSA.permute(vecs_f2)\n",
    "permute_f3 = VSA.permute(vecs_f3)\n",
    "\n",
    "bound_f2_ca = VSA.ca90(bound_f1)\n",
    "bound_f3_ca = VSA.ca90(bound_f2_ca)\n",
    "\n",
    "permute_f2_ca = VSA.ca90(permute_f1)\n",
    "permute_f3_ca = VSA.ca90(permute_f2_ca)\n",
    "\n",
    "print((bound_f2 == bound_f2_ca).all())\n",
    "print((bound_f3 == bound_f3_ca).all())\n",
    "print((permute_f2 == permute_f2_ca).all())\n",
    "print((permute_f3 == permute_f3_ca).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test how CA90 reserves similarity for locality-preserving encoding.\n",
    "## What are the similaries between the full hypervectors if we apply a similarity step function to the seeds only.\n",
    "## Kleyko's paper shows the power of 2 steps are displaying the best similarity to its original seed (about twice of the original distance)\n",
    "## This will require a much more complicated CA90 update scheme.\n",
    "import torch\n",
    "from vsa import VSA\n",
    "VSA.mode = \"HARDWARE\"\n",
    "FOLD_DIM = 100\n",
    "DIM = 1000\n",
    "NUM_FOLDS = DIM // FOLD_DIM\n",
    "NUM_STEPS = 11\n",
    "STEP = FOLD_DIM // (NUM_STEPS - 1)\n",
    "vecs_f0 = VSA.random(size = (NUM_STEPS, FOLD_DIM))\n",
    "# Set up the vectors such that consecutive items differ by STEP in similarity\n",
    "for i in range(1, NUM_STEPS):\n",
    "    vecs_f0[i] = vecs_f0[i-1]\n",
    "    vecs_f0[i][STEP*(i-1):STEP*i] = VSA.inverse(vecs_f0[i][STEP*(i-1):STEP*i])\n",
    "    print(f\"Similarity between fold 0 of vec0 and vec{i} =\", VSA.hamming_similarity(vecs_f0[0], vecs_f0[i]))\n",
    "# print(f\"Similarity between fold 0 of vec0 and vec{NUM_STEPS-1} =\", VSA.hamming_similarity(vecs_f0[0], vecs_f0[NUM_STEPS-1]) / FOLD_DIM)\n",
    "print()\n",
    "\n",
    "# CA90 updates\n",
    "def gen_full_vector(fold):\n",
    "    \"\"\"\n",
    "    Generate the rest of the vector through CA90\n",
    "    \"\"\"\n",
    "    assert(fold.size(-1) == FOLD_DIM)\n",
    "\n",
    "    vector = fold.clone()\n",
    "    for i in range(DIM // FOLD_DIM - 1):\n",
    "        fold = VSA.ca90(fold)\n",
    "        vector = torch.cat((vector, fold), dim=-1)\n",
    "    return vector\n",
    "\n",
    "vecs_full = gen_full_vector(vecs_f0)\n",
    "\n",
    "MID_FOLD = 8\n",
    "for i in range(1, NUM_STEPS):\n",
    "    print(f\"Similarity between fold {MID_FOLD} of vec0 and vec{i} =\", VSA.hamming_similarity(vecs_full[0][MID_FOLD*FOLD_DIM:(MID_FOLD+1)*FOLD_DIM], vecs_full[i][MID_FOLD*FOLD_DIM:(MID_FOLD+1)*FOLD_DIM]))\n",
    "print()\n",
    "\n",
    "for i in range(1, NUM_STEPS):\n",
    "    print(f\"Full similarity bewteen vec0 and vec{i} =\", VSA.hamming_similarity(vecs_full[0], vecs_full[i]))\n",
    "# print(f\"Full similarity bewteen vec0 and vec{NUM_STEPS-1} =\", VSA.hamming_similarity(vecs_full[0], vecs_full[NUM_STEPS-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating codebooks...Done. Saved to tests/folding/codebooks.pt\n",
      "[(7, 4, 2, 7)]\n",
      "Original Full Vector\n",
      "((7, 4, 2, 7), 7, 'EARLY')\n",
      "Similarity after noise: tensor(1.)\n",
      "Noisy Full Vector\n",
      "((7, 4, 2, 7), 13, 'EARLY')\n",
      "Similarity CA90 Expanded Full Vector: tensor(1.)\n",
      "((7, 4, 2, 7), 25, 'EARLY')\n",
      "Fold 0 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 0 Similarity ca against noisy query tensor(1.)\n",
      "Fold 1 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 1 Similarity ca against noisy query tensor(1.)\n",
      "Fold 2 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 2 Similarity ca against noisy query tensor(1.)\n",
      "Fold 3 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 3 Similarity ca against noisy query tensor(1.)\n",
      "Fold 4 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 4 Similarity ca against noisy query tensor(1.)\n",
      "Fold 5 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 5 Similarity ca against noisy query tensor(1.)\n",
      "Fold 6 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 6 Similarity ca against noisy query tensor(1.)\n",
      "Fold 7 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 7 Similarity ca against noisy query tensor(1.)\n",
      "Fold 8 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 8 Similarity ca against noisy query tensor(1.)\n",
      "Fold 9 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 9 Similarity ca against noisy query tensor(1.)\n",
      "Fold 10 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 10 Similarity ca against noisy query tensor(1.)\n",
      "Fold 11 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 11 Similarity ca against noisy query tensor(1.)\n",
      "Fold 12 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 12 Similarity ca against noisy query tensor(1.)\n",
      "Fold 13 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 13 Similarity ca against noisy query tensor(1.)\n",
      "Fold 14 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 14 Similarity ca against noisy query tensor(1.)\n",
      "Fold 15 Similarity ca against noise-free query tensor(1.)\n",
      "Fold 15 Similarity ca against noisy query tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# TODO: There's a obscure bug here. The unnoisy version of the input sometimes fails to produce the correct answer.\n",
    "# The same setup has been tested in the main program and the accuracy is 100% all the time.\n",
    "# Not sure whether it's a typo or a bug hidden in the code\n",
    "\n",
    "import torch\n",
    "from vsa import VSA\n",
    "from dataset import VSADataset\n",
    "import random\n",
    "from vsa.resonator import Resonator\n",
    "# Generate codebooks of 1st fold only\n",
    "root = \"tests/folding\"\n",
    "N_SUPERPOSED = 1\n",
    "NUM_FACTORS = 4\n",
    "FOLD_DIM = 128\n",
    "DIM = 2048\n",
    "NUM_FOLDS = DIM // FOLD_DIM\n",
    "NOISE = 0\n",
    "vsa = VSA(root=root, mode=\"HARDWARE\", dim=DIM, fold_dim = FOLD_DIM, ehd_bits=9, num_factors=NUM_FACTORS, num_codevectors=8, seed=0)\n",
    "# ds = VSADataset(\"test/folding\", 10, vsa, algo=\"ALGO1\", num_vectors_superposed=NUM_VEC_SUPERPOSED, quantize=q, noise=n, device=device)\n",
    "\n",
    "labels = [tuple([random.randint(0, len(vsa.codebooks[i])-1) for i in range(NUM_FACTORS)]) for j in range(N_SUPERPOSED)]\n",
    "print(labels)\n",
    "\n",
    "rn = Resonator(vsa=vsa, mode=\"HARDWARE\", type=\"SEQUENTIAL\", activation=\"THRESHOLD\", iterations=5000, \n",
    "               argmax_abs=True, act_val=8, stoch=\"SIMILARITY\", \n",
    "               randomness=0.04, early_converge=0.8, seed=0)\n",
    "\n",
    "_query = vsa.get_vector(labels, quantize=True)\n",
    "print(\"Original Full Vector\")\n",
    "print(rn(_query, rn.get_init_estimates()))\n",
    "\n",
    "query = vsa.apply_noise(_query, NOISE)\n",
    "print(f\"Similarity after noise:\", vsa.hamming_similarity(query, _query) / DIM)\n",
    "print(\"Noisy Full Vector\")\n",
    "print(rn(query, rn.get_init_estimates()))\n",
    "\n",
    "cb_fold = [0] * NUM_FOLDS\n",
    "query_fold = [0] * NUM_FOLDS\n",
    "for i in range(NUM_FOLDS):\n",
    "    cb_fold[i] = vsa.codebooks[:,:,i*FOLD_DIM:(i+1)*FOLD_DIM]\n",
    "    # query_fold[i] = vsa.get_vector(labels, codebooks = cb_fold[i], quantize = True)\n",
    "    query_fold[i] = query[i*FOLD_DIM:(i+1)*FOLD_DIM] # Noisy version\n",
    "# CA90 expands from noisy first fold\n",
    "query_ca = vsa._gen_full_vector(query_fold[0])\n",
    "\n",
    "init_est_full = rn.get_init_estimates()\n",
    "print(\"Similarity CA90 Expanded Full Vector:\", vsa.hamming_similarity(_query, query_ca) / DIM)\n",
    "print(rn(query_ca, init_est_full))\n",
    "\n",
    "for i in range(NUM_FOLDS):\n",
    "    print(f\"Fold {i} Similarity ca against noise-free query\", vsa.hamming_similarity(_query[i*FOLD_DIM:(i+1)*FOLD_DIM], query_ca[i*FOLD_DIM:(i+1)*FOLD_DIM]) / FOLD_DIM)\n",
    "    print(f\"Fold {i} Similarity ca against noisy query\", vsa.hamming_similarity(query_fold[i], query_ca[i*FOLD_DIM:(i+1)*FOLD_DIM]) / FOLD_DIM)\n",
    "    # print(rn(query_fold[i], rn.get_init_estimates(cb_fold[i]), cb_fold[i]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
